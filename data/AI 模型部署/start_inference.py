# Qwen2.5 æ¨ç†æœåŠ¡å¯åŠ¨è„šæœ¬ï¼ˆTransformers ç‰ˆæœ¬ï¼‰
# ä½¿ç”¨æ–¹æ³•ï¼špython start_inference.py

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
from typing import Optional, List

# ========== é…ç½® ==========
MODEL_PATH = "./models/dental_qwen_merged"  # æ¨¡å‹è·¯å¾„
HOST = "0.0.0.0"
PORT = 8080
MAX_TOKENS = 512
TEMPERATURE = 0.7

# ========== åŠ è½½æ¨¡å‹ ==========
print("=" * 60)
print("ğŸš€ æ­£åœ¨åŠ è½½ç‰™ç§‘ä¿®å¤ AI æ¨¡å‹...")
print("=" * 60)

print(f"æ¨¡å‹è·¯å¾„ï¼š{MODEL_PATH}")

# æ£€æŸ¥ CUDA
if torch.cuda.is_available():
    print(f"âœ… CUDA å¯ç”¨ - GPU: {torch.cuda.get_device_name(0)}")
    device = "cuda"
else:
    print("âš ï¸  ä½¿ç”¨ CPU æ¨ç†ï¼ˆè¾ƒæ…¢ï¼‰")
    device = "cpu"

# åŠ è½½ tokenizer
print("åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    padding_side="left"
)

# åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹æƒé‡...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

model.eval()

print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
print(f"   æ¨¡å‹ç±»å‹ï¼š{model.config.model_type}")
print(f"   è¯è¡¨å¤§å°ï¼š{len(tokenizer)}")
print("=" * 60)


# ========== æ¨ç†å‡½æ•° ==========
def generate_response(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: int = MAX_TOKENS,
    temperature: float = TEMPERATURE
) -> str:
    """
    ç”Ÿæˆ AI å›å¤
    """
    # æ„å»ºå¯¹è¯æ ¼å¼
    if system_prompt:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
    else:
        messages = [{"role": "user", "content": prompt}]
    
    # åº”ç”¨ chat template
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        temperature=temperature,
        do_sample=True,
        top_p=0.9,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # è§£ç å›å¤
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant å›å¤éƒ¨åˆ†
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    return response


# ========== FastAPI æœåŠ¡ ==========
app = FastAPI(
    title="ç‰™ç§‘ä¿®å¤ AI æ¨ç†æœåŠ¡",
    description="åŸºäº Qwen2.5-7B çš„ç‰™ç§‘ä¿®å¤é¢†åŸŸ AI æ¨ç† API",
    version="1.0.0"
)


class GenerateRequest(BaseModel):
    """ç”Ÿæˆè¯·æ±‚"""
    prompt: str
    system_prompt: Optional[str] = None
    max_tokens: Optional[int] = MAX_TOKENS
    temperature: Optional[float] = TEMPERATURE


class GenerateResponse(BaseModel):
    """ç”Ÿæˆå“åº”"""
    text: str
    model: str = "dental_qwen"


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str
    content: str


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰"""
    messages: List[ChatMessage]
    max_tokens: Optional[int] = MAX_TOKENS
    temperature: Optional[float] = TEMPERATURE


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰"""
    choices: List[dict]
    model: str = "dental_qwen"


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ç‰™ç§‘ä¿®å¤ AI æ¨ç†æœåŠ¡å·²å¯åŠ¨",
        "model": "dental_qwen",
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "cpu",
        "memory_used": f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB" if torch.cuda.is_available() else "N/A"
    }


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    ç®€å•ç”Ÿæˆæ¥å£
    
    - **prompt**: ç”¨æˆ·è¾“å…¥
    - **system_prompt**: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
    - **max_tokens**: æœ€å¤§ç”Ÿæˆ token æ•°
    - **temperature**: æ¸©åº¦å‚æ•°
    """
    try:
        text = generate_response(
            prompt=request.prompt,
            system_prompt=request.system_prompt,
            max_tokens=request.max_tokens or MAX_TOKENS,
            temperature=request.temperature or TEMPERATURE
        )
        return GenerateResponse(text=text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completions(request: ChatRequest):
    """
    OpenAI å…¼å®¹æ ¼å¼çš„èŠå¤©æ¥å£
    
    - **messages**: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/system", "content": "..."}]
    - **max_tokens**: æœ€å¤§ç”Ÿæˆ token æ•°
    - **temperature**: æ¸©åº¦å‚æ•°
    """
    try:
        # æå–ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
        system_prompt = None
        user_message = None
        
        for msg in request.messages:
            if msg.role == "system":
                system_prompt = msg.content
            elif msg.role == "user":
                user_message = msg.content
        
        if not user_message:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘ç”¨æˆ·æ¶ˆæ¯")
        
        text = generate_response(
            prompt=user_message,
            system_prompt=system_prompt,
            max_tokens=request.max_tokens or MAX_TOKENS,
            temperature=request.temperature or TEMPERATURE
        )
        
        return ChatResponse(
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": text
                },
                "finish_reason": "stop"
            }]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/test")
async def test_inference():
    """æµ‹è¯•æ¨ç†åŠŸèƒ½"""
    test_questions = [
        "ç§æ¤ç‰™æœ¯åå¤šä¹…èƒ½åƒé¥­ï¼Ÿ",
        "æ´»åŠ¨ä¹‰é½¿åˆšæˆ´ä¸Šå¾ˆä¸èˆ’æœï¼Œæ­£å¸¸å—ï¼Ÿ",
        "çƒ¤ç“·ç‰™èƒ½ç”¨å¤šä¹…ï¼Ÿ"
    ]
    
    results = []
    for q in test_questions:
        try:
            answer = generate_response(q)
            results.append({
                "question": q,
                "answer": answer[:100] + "..." if len(answer) > 100 else answer
            })
        except Exception as e:
            results.append({
                "question": q,
                "error": str(e)
            })
    
    return {"test_results": results}


if __name__ == "__main__":
    print("\nâœ… å‡†å¤‡å¯åŠ¨æ¨ç†æœåŠ¡...")
    print(f"   è®¿é—®åœ°å€ï¼šhttp://localhost:{PORT}")
    print(f"   API æ–‡æ¡£ï¼šhttp://localhost:{PORT}/docs")
    print(f"   å¥åº·æ£€æŸ¥ï¼šhttp://localhost:{PORT}/health")
    print(f"   æµ‹è¯•æ¥å£ï¼šhttp://localhost:{PORT}/test")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
    
    uvicorn.run(
        app,
        host=HOST,
        port=PORT,
        log_level="info"
    )
