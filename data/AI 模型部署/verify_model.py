# AutoDL æ¨¡å‹éªŒè¯è„šæœ¬
# ä½¿ç”¨æ–¹æ³•ï¼šåœ¨ AutoDL JupyterLab ç»ˆç«¯æ‰§è¡Œ python verify_model.py

import os
import torch

print("=" * 60)
print("ğŸ” AutoDL æ¨¡å‹éªŒè¯è„šæœ¬")
print("=" * 60)

# 1. æ£€æŸ¥å¸¸è§æ¨¡å‹è·¯å¾„
print("\nğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ä½ç½®...\n")

possible_paths = [
    "./dental_qwen_merged",
    "./models/dental_qwen_merged",
    "/root/autodl-tmp/dental_qwen_merged",
    "/root/autodl-tmp/models/dental_qwen_merged",
    "./checkpoints/dental_qwen_lora",
    "/root/autodl-tmp/checkpoints/dental_qwen_lora",
]

found_model_path = None
found_lora_path = None

for path in possible_paths:
    if os.path.exists(path):
        print(f"âœ… æ‰¾åˆ°ï¼š{path}")
        
        # åˆ—å‡ºæ–‡ä»¶
        files = os.listdir(path)
        print(f"   æ–‡ä»¶åˆ—è¡¨ï¼š{files[:10]}{'...' if len(files) > 10 else ''}")
        
        # åˆ¤æ–­æ˜¯å®Œæ•´æ¨¡å‹è¿˜æ˜¯ LoRA æƒé‡
        if "model.safetensors" in files or "pytorch_model.bin" in files:
            print(f"   ğŸ“¦ ç±»å‹ï¼šå®Œæ•´æ¨¡å‹")
            found_model_path = path
        elif "adapter_model.safetensors" in files or "adapter_config.json" in files:
            print(f"   ğŸ”§ ç±»å‹ï¼šLoRA æƒé‡")
            found_lora_path = path
        print()
    else:
        print(f"âŒ ä¸å­˜åœ¨ï¼š{path}")

# 2. æ‰«æ autodl-tmp ç›®å½•ä¸‹æ‰€æœ‰å¯èƒ½çš„ç›¸å…³æ–‡ä»¶
print("\n" + "=" * 60)
print("ğŸ” æ‰«æ /root/autodl-tmp/ ç›®å½•...\n")

autodl_tmp = "/root/autodl-tmp"
if os.path.exists(autodl_tmp):
    for root, dirs, files in os.walk(autodl_tmp):
        # è·³è¿‡ .git å’Œå…¶ä»–éšè—ç›®å½•
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹ç›¸å…³æ–‡ä»¶
        model_keywords = ["dental", "qwen", "lora", "adapter", "merged", "checkpoint"]
        folder_name = os.path.basename(root).lower()
        
        if any(kw in folder_name for kw in model_keywords):
            print(f"ğŸ“‚ æ‰¾åˆ°ç›¸å…³ç›®å½•ï¼š{root}")
            
            # åˆ—å‡ºå‰ 10 ä¸ªæ–‡ä»¶
            relevant_files = [f for f in files if f.endswith(('.json', '.bin', '.safetensors'))]
            if relevant_files:
                print(f"   å…³é”®æ–‡ä»¶ï¼š{relevant_files[:10]}")
            
            # ä¼°ç®—ç›®å½•å¤§å°
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(root):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    if os.path.exists(fp):
                        total_size += os.path.getsize(fp)
            
            size_gb = total_size / (1024**3)
            print(f"   å¤§å°ï¼š{size_gb:.2f} GB")
            print()

# 3. æ£€æŸ¥ GPU çŠ¶æ€
print("=" * 60)
print("ğŸ–¥ï¸ GPU çŠ¶æ€æ£€æŸ¥...\n")

if torch.cuda.is_available():
    print(f"âœ… CUDA å¯ç”¨")
    print(f"   GPU å‹å·ï¼š{torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜æ€»é‡ï¼š{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"   æ˜¾å­˜å·²ç”¨ï¼š{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    print(f"   æ˜¾å­˜ç©ºé—²ï¼š{torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
else:
    print("âŒ CUDA ä¸å¯ç”¨")

# 4. æ£€æŸ¥ä¾èµ–
print("\n" + "=" * 60)
print("ğŸ“¦ ä¾èµ–æ£€æŸ¥...\n")

dependencies = {
    "torch": "PyTorch",
    "transformers": "Transformers",
    "vllm": "vLLM",
    "peft": "PEFT (LoRA)",
    "accelerate": "Accelerate",
}

for pkg, name in dependencies.items():
    try:
        mod = __import__(pkg)
        version = getattr(mod, "__version__", "unknown")
        print(f"âœ… {name}: {version}")
    except ImportError:
        print(f"âŒ {name}: æœªå®‰è£…")

# 5. æ€»ç»“
print("\n" + "=" * 60)
print("ğŸ“Š éªŒè¯æ€»ç»“")
print("=" * 60)

if found_model_path:
    print(f"\nâœ… å®Œæ•´æ¨¡å‹å·²æ‰¾åˆ°ï¼š{found_model_path}")
    print(f"   å¯ä»¥ç›´æ¥ç”¨äºéƒ¨ç½²ï¼")

if found_lora_path:
    print(f"\nâœ… LoRA æƒé‡å·²æ‰¾åˆ°ï¼š{found_lora_path}")
    print(f"   éœ€è¦ä¸åŸºåº§æ¨¡å‹åˆå¹¶åä½¿ç”¨")

if found_model_path and found_lora_path:
    print(f"\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ LLaMA Factory åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹")
    print(f"   å‘½ä»¤ç¤ºä¾‹:")
    print(f"   llamafactory-cli export \\")
    print(f"       --model_name_or_path Qwen/Qwen2.5-7B-Instruct \\")
    print(f"       --adapter_name_or_path {found_lora_path} \\")
    print(f"       --export_dir ./dental_qwen_merged \\")
    print(f"       --template qwen")
elif not found_model_path and not found_lora_path:
    print(f"\nâŒ æœªæ‰¾åˆ°æ¨¡å‹æˆ– LoRA æƒé‡")
    print(f"   è¯·æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ åˆ° AutoDL æœåŠ¡å™¨")
    print(f"   å¸¸è§ä½ç½®ï¼š/root/autodl-tmp/dental_qwen_merged/")

print("\n" + "=" * 60)
